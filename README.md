# PrÃ¡ctica con Licitaciones: RAG-Powered Conversational Analyzer

Una API en **FastAPI** que combina LangChain, ChromaDB y OpenAI para ofrecer un **chat conversacional** con RAG (Retrieval-Augmented Generation) sobre documentos de licitaciones en PDF.

> ğŸ† **MVP de hackathon**

---

## ğŸš€ CaracterÃ­sticas

* **Indexado offline** de mÃºltiples licitaciones (PDF) en un vector-store.
* **Chunking inteligente** con `RecursiveCharacterTextSplitter`.
* **Vector-store** persistente utilizando ChromaDB.
* **Conversational RAG** mediante `ConversationalRetrievalChain` de LangChain.
* **Memoria de diÃ¡logo** para mantener el contexto entre turnos de chat.
* **Metadata & trazabilidad**: cada fragmento indexado incluye nombre de archivo, pÃ¡gina y chunk\_id. *(Pendiente de implementaciÃ³n completa)*
* **API REST** con un Ãºnico endpoint `POST /chat`.

---

## ğŸ“¦ Prerrequisitos

* Python 3.10 o superior
* Clave de API de OpenAI
* (Opcional) Docker y Docker Hub

---

## ğŸ”§ InstalaciÃ³n y configuraciÃ³n

1. **Clonar el repositorio**

   ```bash
   git clone https://github.com/tu-org/hackia-licita.git
   cd hackia-licita/backend
   ```

2. **Crear y activar el entorno virtual**

   ```bash
   python3 -m venv .venv

   # macOS / Linux
   source .venv/bin/activate

   # Windows PowerShell
   .venv\Scripts\Activate.ps1
   ```

3. **Instalar dependencias**

   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar variables de entorno**
   Copia `.env.example` a `.env` y edita:

   ```ini
   OPENAI_API_KEY=sk-...
   CHROMA_PERSIST=.chromadb
   ```

---

## ğŸ“ Estructura del proyecto

```plaintext
backend/
â”œâ”€â”€ docs/                  # Licitaciones (PDF/TXT)
â”‚   â”œâ”€â”€ ejemplo1.pdf
â”‚   â””â”€â”€ ejemplo2.pdf
â”œâ”€â”€ .chromadb/             # Vector-store de ChromaDB (generado)
â”œâ”€â”€ .env                   # Variables sensibles (no versionar)
â”œâ”€â”€ model_clf.pkl          # (Opcional) Clasificador supervisado
â”œâ”€â”€ requirements.txt       # Dependencias del proyecto
â”œâ”€â”€ README.md              # DocumentaciÃ³n principal
â””â”€â”€ src/                   # CÃ³digo fuente
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config.py         # Carga de variables de entorno
    â”œâ”€â”€ ingestion.py      # IndexaciÃ³n de documentos en ChromaDB
    â”œâ”€â”€ chain.py          # DefiniciÃ³n del RAG-Chat chain
    â”œâ”€â”€ main.py           # FastAPI: expone `POST /chat`
    â””â”€â”€ train_classifier.py  # (Opcional) Entrenamiento del clasificador
```

---

## âš™ï¸ Flujo de trabajo

### 1. IndexaciÃ³n

Antes de levantar la API, indexa todos los documentos:

```bash
python -m src.ingestion
```

**Salida esperada:**

```text
ğŸ” Buscando archivos en: backend/docs
â–¶ï¸ Cargando ejemplo1.pdf
â–¶ï¸ Cargando ejemplo2.pdf
âœ‚ï¸ Fragmentos creados: 24
âœ… Indexados 24 fragmentos de 2 archivos.
```

### 2. (NO IMPLEMENTADO) Entrenamiento del clasificador

Para generar un modelo supervisado de scoring de riesgo:

```bash
python -m src.train_classifier
```

Se crearÃ¡ el archivo `model_clf.pkl` en la raÃ­z.

### 3. Levantar la API

```bash
uvicorn src.main:app --reload
```

* Swagger UI: `http://127.0.0.1:8000/docs`

### 4. Probar el chat

```bash
curl -X POST http://127.0.0.1:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"Â¿QuÃ© penalizaciones hay por retrasos?"}'
```

**Respuesta de ejemplo:**

```json
{
  "answer": "El contrato establece una penalizaciÃ³n de 0.5% del valor diario por dÃ­a de retraso en la entrega de luminarias.",
  "sources": ["ejemplo1.pdf"]
}
```

---

## ğŸ› ï¸ Detalles de implementaciÃ³n

* **`ingestion.py`**:

  * Usa `RecursiveCharacterTextSplitter` para generar chunks de \~500 caracteres.
  * Inyecta metadata (`source`, `page`, `chunk_id`).
  * Indexa los fragments en ChromaDB.

* **`chain.py`**:

  * Configura `OpenAIEmbeddings` + Chroma retriever (`search_kwargs={"k":4}`).
  * Define `ChatOpenAI(model_name="gpt-4o-mini")`.
  * Construye `ConversationalRetrievalChain.from_llm(...)` con memoria (solo guarda `output_key="answer"`).

* **`main.py`**:

  * Esquemas Pydantic: `ChatReq` y `ChatRes(answer: str, sources: List[str])`.
  * Un Ãºnico endpoint `POST /chat`.

---

## ğŸ“‘ Contribuciones

1. Haz **fork** de este repositorio.
2. Crea una rama para tu mejora:

   ```bash
   ```

git checkout -b feature/mi-mejora

```
3. AÃ±ade tests y documentaciÃ³n si es necesario.  
4. Abre un **Pull Request** describiendo tu aporte.

```


